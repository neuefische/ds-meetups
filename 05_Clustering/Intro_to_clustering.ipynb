{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Clustering with K-Means"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \"If the intelligence is a cake the bulk of the cake is unsupervised learning, the icing on the cake is supervised learning and the cherry on the cake is reinforcement learning.\" Yann LeCun (NIPS 2016)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will showcase how to cluster data with the K-Means algorithm. We will use scikit-learn's implementation and apply it to the fashion MNIST dataset.\n",
    "\n",
    "**Notebook Outline:**\n",
    "* load data and EDA\n",
    "* pre-process data for clustering\n",
    "* determine appropriate number of clusters using Yellowbrick library\n",
    "* cluster data using K-Means\n",
    "* evaluate clustering performance and perform error analysis\n",
    "\n",
    "\n",
    "**Notebook Objectives:**\n",
    "\n",
    "At the end of the notebook, you should...\n",
    "* be familiar with the fashion MNIST data set\n",
    "* know how to apply the elbow method\n",
    "* be able to cluster data using scikit-learn's K-Means implementation\n",
    "* understand the difficulties when it comes to evaluating clustering performance\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import rand_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow.keras.datasets import fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's define some colors (yaaay corporate identity!)\n",
    "NF_Nemo=\"#FF4A11\"\n",
    "NF_Granite=\"#252629\"\n",
    "#NF_Foam=\"#FFFFFF\"\n",
    "\n",
    "#NF_DarkNemo=\"#E5430F\"\n",
    "#NF_LightNemo=\"#FFB7A0\"\n",
    "#NF_Dark_Grey=\"#3F4145\"\n",
    "\n",
    "NF_Blue=\"#33A5FF\"\n",
    "#NF_Green=\"#A7E521\"\n",
    "#NF_Yellow=\"#FFE600\"\n",
    "#NF_Purple=\"#696CFF\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "The [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset contains 70,000 28x28 greyscale images of fashion products from a dataset of Zalando article images. It consists of 10 different classes which 7000 images per category. It is freely available for training and testing machine learning applications and can be directly downloaded from Tensorflow.\n",
    "\n",
    "The dataset introduced by Zalando is a more contemporary version of the original MNIST dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data from Tensorflow\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "print('Fashion MNIST Dataset Shape:')\n",
    "print('X_train: ' + str(X_train.shape))\n",
    "print('y_test: ' + str(y_test.shape))\n",
    "print('X_test:  '  + str(X_test.shape))\n",
    "print('y_test:  '  + str(y_test.shape))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we have 60,000 samples in the training set (-> X_train) with a dimension of 28x28 each. Additionally, we got 10,000 samples for the testing set(-> X_test) with the same dimensions.\n",
    "\n",
    "The Dataset was designed to be used for supervised learning, so we also get the correct labels (in this case class labels) for all of the samples in the training (-> y_train) and test (-> y_test) sets. Typically, clustering is used in unsupervised settings. Then, we wouldn't have the labels available. For the purpose of this meetup, however we will use them to evaluate how good our clustering is working.\n",
    "\n",
    "The correct class names are encoded as numbers 0-9 but we can use the following dictionary to turn it into human readable labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class labels as dictionary \n",
    "class_names_dict = {0: 'T-shirt/top', 1: 'Trouser', 2: 'Pullover', 3: 'Dress', 4: 'Coat',\n",
    "               5: 'Sandal', 6: 'Shirt', 7: 'Sneaker', 8: 'Bag', 9: 'Ankle boot'}\n",
    "\n",
    "display(class_names_dict)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, as we can see we get some different items that most of us have in their wardrobes. Let's have a closer look at the data that we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'This is the first item: \\n\\n {X_train[0]} \\n\\n It has the dimension: {X_train[0].shape}. \\n It has the label {y_train[0]} assigned, which means it is a {class_names_dict[y_train[0]]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you recognise the boot from that numbers? Me neither. We know it is supposed to be a picture, so let's treat it like a picture instead. The numbers in the data (0 to 255) represent the color of each of the 28x28 pixels we have. We can use matplotlib's [plt.imshow()](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html) for a better presentation - we just add some custom labels, colors etc. to improve the visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom plotting function\n",
    "def custom_imshow(image, ax, title, aspect='equal', show_minor=False, show_major=False):\n",
    "    # Show image\n",
    "    im = ax.imshow(image,\n",
    "                    cmap=\"gray\",\n",
    "                    interpolation='none', \n",
    "                    vmin=0, \n",
    "                    vmax=255, \n",
    "                    aspect=aspect)\n",
    "    \n",
    "    # Add title\n",
    "    ax.set_title(title)\n",
    "\n",
    "    # Add (or don't) the selected grid lines\n",
    "    if(show_minor):\n",
    "        # Add Minor ticks (for the minor grid)\n",
    "        ax.set_xticks(np.arange(-.5, 28, 1), minor=True)\n",
    "        ax.set_yticks(np.arange(-.5, 28, 1), minor=True)\n",
    "\n",
    "        # Remove Minor ticks\n",
    "        ax.tick_params(which='minor', bottom=False, left=False)\n",
    "        ax.grid(which='minor', color=NF_Blue, linestyle='-', linewidth=1)\n",
    "    else:\n",
    "        ax.grid(which='minor',visible=False)\n",
    "        \n",
    "    if(show_major):\n",
    "        ax.grid(which='major', color=NF_Nemo, linestyle='-', linewidth=1)\n",
    "    else:\n",
    "        ax.grid(which='major',visible=False)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get a proper visualisation of our sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'This is the first item:')\n",
    "plt.figure(figsize=(8,8))\n",
    "ax=plt.gca()\n",
    "custom_imshow(X_train[0],ax,title=\"\",show_minor=True,show_major=False)\n",
    "plt.show()\n",
    "print(f'It has the dimension: {X_train[0].shape}. \\nIt has the label {y_train[0]} assigned, which means it is a {class_names_dict[y_train[0]]}.')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that starts to make more sense! Let's look at some more observations!\n",
    "\n",
    "Do you notice something that could be problematic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First fix random seed so that everyone using this sees the same picture. Feel free to change it though!\n",
    "random_seed=42\n",
    "\n",
    "# specify the number of rows and columns you want to see\n",
    "num_row = 3\n",
    "num_col = 5\n",
    "\n",
    "# Here we compute how many samples we want:\n",
    "num = num_row*num_col \n",
    "\n",
    "# and here we use sklearn's resample function to draw the sample\n",
    "images, labels, original_index=resample(X_train,\n",
    "                                        y_train,\n",
    "                                        range(len(y_train)),\n",
    "                                        n_samples=num,\n",
    "                                        random_state=random_seed) \n",
    "\n",
    "# plot images\n",
    "fig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))\n",
    "for i in range(num):\n",
    "    if num_row>1 and num_col>1:\n",
    "        ax = axes[i//num_col, i%num_col]\n",
    "    elif num >1:\n",
    "        ax = axes[i]\n",
    "    else: ax=axes\n",
    "    #(image,ax,title,show_minor=False,show_major=False):\n",
    "    custom_imshow(images[i],ax,f'Item: {original_index[i]}\\nLabel: {labels[i]}\\n{class_names_dict[labels[i]]}',show_minor=False,show_major=False)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the items, e.g. ankle boots and trousers look pretty different that should be fine. Others, like for example shirts, pullovers and coats, however,look pretty similar. Let's see if our clustering algorithm will be able to separate them correctly."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's see how different items from each of the different classes look like: Is there something coming to your mind?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select which class to look at\n",
    "class_number=8\n",
    "\n",
    "# specify the number of rows and columns you want to see\n",
    "num_row = 2\n",
    "num_col = 5\n",
    "\n",
    "# Here we compute how many samples we want:\n",
    "num = num_row*num_col \n",
    "\n",
    "indices_for_class=np.argwhere(y_train==class_number).flatten() # get all the indices that are samples of the class we are interested in\n",
    "images = X_train[indices_for_class][:num] # filter for the indices and only get as many as we need\n",
    "labels = y_train[indices_for_class][:num]\n",
    "original_index =indices_for_class[:num]   # keep original index, so that we can find the items again\n",
    "\n",
    "# plot images\n",
    "fig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))\n",
    "for i in range(num):\n",
    "    if num_row>1 and num_col>1:\n",
    "        ax = axes[i//num_col, i%num_col]\n",
    "    elif num >1:\n",
    "        ax = axes[i]\n",
    "    else: ax=axes\n",
    "    #(image,ax,title,show_minor=False,show_major=False):\n",
    "    custom_imshow(images[i],ax,f'Item: {original_index[i]}\\nLabel: {labels[i]}\\n{class_names_dict[labels[i]]}',show_minor=False,show_major=False)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the classes, e.g. the bags (label 8) contain items that look pretty dissimilar. On the other hand some classes look much more uniform, e.g. trousers (label 1)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process Data for clustering\n",
    "\n",
    "After getting a feeling for the data we are dealing with, we can start to bring the data into the correct format. Scikit-learn's `K-Means` implementation expects the data to be an array with the shape (n_samples, n_features). Currently our data has one dimension to much, so we will need to flatten the pixels for each picture changing the shape from 28 x 28 (width x height) to a flat vector of 784 pixels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current shape of data (number of sample, width, height)\n",
    "print(\"Current shape X_train: \", X_train.shape)\n",
    "print(\"Current shape X_test:  \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping data using numpy\n",
    "X_train_flat = X_train.reshape(60000, -1)\n",
    "X_test_flat = X_test.reshape(10000, -1)\n",
    "\n",
    "print(\"New shape X_train: \", X_train_flat.shape)\n",
    "print(\"New shape X_test:  \", X_test_flat.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize what happened when we reshaped our data. The following plots compare the original 2-dimensional picture of one observation with the flattened array. You can think of it as cutting the picture horizontally in thin stripes and glueing them together to form a long stripe of pixels. \n",
    "\n",
    "> Note: To be able to see the color difference in the flattened image, we changed the aspect ratio. Ech pixel is not shown as a square but stretched vertically. If you want to display the original line of square pixels, you can change the parameter `aspect=\"auto\"` to `aspect=\"equal\"`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to see another observation change the value of the observation variable\n",
    "observation = 42\n",
    "\n",
    "fig, ax = plt.subplot_mosaic(\"A..;A..;A..;A..;BBB\", figsize=(15, 6))\n",
    "custom_imshow(X_train[observation], ax=ax['A'], title=\"2-D image\", show_minor=False, show_major=False)\n",
    "custom_imshow(X_train_flat[observation].reshape(-1,1).T, ax=ax['B'], title=\"Flattened image\", aspect='auto', show_minor=False, show_major=False)\n",
    "plt.suptitle(\"Comparison between 2-D image and flattened image of the same observation\")\n",
    "plt.subplots_adjust(hspace=0.5);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our data has the correct shape, we can continue with the actual clustering process. Before we train our model, let's draw a random sample that consists of 6,000 of the original 60,000 pictures of the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw random sample from training data using scikit-learn's resample function\n",
    "X_sample_flat, y_sample = resample(X_train_flat, y_train, n_samples=6000, replace=False, stratify=y_train, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print shape of sample data\n",
    "print(\"X_sample shape: \", X_sample_flat.shape)\n",
    "print(\"y_sample shape: \", y_sample.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply K-Means and Elbow Method\n",
    "\n",
    "Finally we can cluster our data using K-Means. As you might remember, after initializing the cluster centroids K-Means alternates between two steps: \n",
    "1. assigning the data points to the nearest cluster centroid and \n",
    "2. updating the location of the centroids by calculating the mean of the data points assigned to each cluster \n",
    "\n",
    "until the assignment of the data points to the clusters doesn't change anymore. \n",
    "\n",
    "One major drawback of the algorithm is, that it cannot determine the amount of clusters itself. We need to define it when we fit the model. Sometimes the amount of clusters in our data set is given by the specific business case we are trying to solve (e.g. clustering customer data into a certain amount of groups), or we can gain some intuition during the EDA. But it can also happen, that we simply don't know what an appropriate amount of clusters would be. In those cases the elbow method can help us to select a value for k. \n",
    "\n",
    "In the next cell we will use the [yellowbrick](https://www.scikit-yb.org/en/latest/) package to create an elbow plot. We will run K-Means with a different amount of clusters and visualise the within-cluster sum of squares over the number of clusters (blue solid line). Yellowbrick automatically also plots the fit time in seconds as a secondary metric (light green, dashed line). Since running the algorithm a couple of times on our whole training data would take some time, we will use the random sample we created.\n",
    "\n",
    "> Note: It might seem counterintuitive that we use a method to determine the number of clusters in our data even if we already know that we have ten different categories. Keep in mind, that this is usually not the case! Clustering is a domain of **unsupervised learning**, where we have to deal with **unlabeled** data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use elbow method with sample dataset to determine value for k \n",
    "kmeans_elbow_method = KMeans(init=\"k-means++\", n_init=\"auto\", random_state=1)\n",
    "visualizer_elbow_method = KElbowVisualizer(kmeans_elbow_method, k=(5,20))\n",
    "\n",
    "# Fit visualizer to sample data and display plot\n",
    "visualizer_elbow_method.fit(X_sample_flat)   \n",
    "visualizer_elbow_method.show();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yellowbrick's elbow plot indicates with the dashed black line, that `k=11` is the best amount of clusters for our data. It is the point of maximum curvature of the blue curve showing the distortion score (aka. within-cluster sum of squares). Let's adopt the suggestion and fit `KMeans()` with 11 cluster. Besides the parameter for the amount of clusters `n_clusters` we can also define which initialisation method for placing the initial cluster centroids we want to use. We will use the default value `init=k-means++`, which assures that the centroids get placed as far away form each other as possible mitigating the risk of accidentally splitting a cluster in half. With the third parameter `n_init`, we can control how many times K-Means will be run with different starting position for the centroids. Using `auto` in combination with `k-means++` results in one run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train K-Means on whole training data\n",
    "kmeans = KMeans(n_clusters=11 , init=\"k-means++\", n_init=\"auto\", random_state=0)\n",
    "kmeans.fit(X_train_flat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Performance and Error Analysis\n",
    "\n",
    "Finally, we can use our fitted K-Means model to create labels for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with fitted K-Means\n",
    "y_pred_train = kmeans.predict(X_train_flat)\n",
    "y_pred_test = kmeans.predict(X_test_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Rand index\n",
    "print(\"Score on train set: \", rand_score(y_train, y_pred_train).round(4))\n",
    "print(\"Score on test set:  \", rand_score(y_test, y_pred_test).round(4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we don't only know the predicted labels but also the true ones, we can calculate the Rand index, which measures the similarity between both, ignoring permutations. It can range between 0 and 1, where 1 indicates perfect labeling. With a Rand index of ~0.885 our clustering works in general quite well. However, we already know that there might be a deviation in the number of clusters. Our data has originally 10 categories but the algorithm was trained using 11 clusters.\n",
    "\n",
    "Also, during the initial EDA, we suspected that some classes might be harder to separate then others. Let's see if we were correct with that assumption!\n",
    "\n",
    "Since K-Means is an unsupervised learning method, it randomly applies cluster numbers. Let's see, how well they match the classes we had to begin with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cluster size \n",
    "sns.histplot(y_pred_train, color=NF_Granite)\n",
    "sns.lineplot(x=[-1,11], y=[6000,6000], c=NF_Nemo, ls='--')\n",
    "plt.xlim(-1, 11)\n",
    "plt.xticks(range(0,11))\n",
    "plt.xlabel(\"Cluster number\")\n",
    "plt.title(\"Amount of observations in each cluster\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that our training dataset consists of 6,000 observations for each class. K-Means, however assigned over 8,000 observations to cluster 0, and only between 2,000 and 3,000 to cluster 8. This is already showing that our model was not able to cluster all observations correctly.\n",
    "\n",
    "Next, let's investigate where the different observations ended up by looking at a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix\n",
    "matrix = confusion_matrix(y_train, y_pred_train)\n",
    "sns.heatmap(matrix, square=True, annot=True, fmt='d')\n",
    "plt.xlabel('predicted label')\n",
    "plt.ylabel('true label');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the clustering not working correctly, the assigned labels are permuted. E.g. the vast majority of observations in the predicted cluster 6 belong to the true label 1. \n",
    "\n",
    "To correct this, we can use `np.argmin()` to select for each column the highest count to generate a \"translation dictionary\" between the assigned cluster labels and the original true labels. If we apply this, the resulting confusion matrix looks at least a bit better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_dictionary = dict(zip(range(11), np.argmax(matrix, axis=0))) # pred label 1 and 4 end up being label 3\n",
    "\n",
    "matrix_new = confusion_matrix(y_train, [prediction_dictionary[i] for i in y_pred_train])\n",
    "sns.heatmap(matrix_new, square=True, annot=True, fmt='d')\n",
    "plt.xlabel('predicted label')\n",
    "plt.ylabel('true label');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select which cluster to look at -> predicted label != class label!\n",
    "cluster_number=5\n",
    "\n",
    "# specify the number of rows and columns you want to see\n",
    "num_row = 4\n",
    "num_col = 5\n",
    "\n",
    "# Here we compute how many samples we want:\n",
    "num = num_row*num_col \n",
    "\n",
    "indices_for_class=np.argwhere(y_pred_train==cluster_number).flatten() #get all the indices that are samples of the class we are interested in\n",
    "images = X_train_flat[indices_for_class][:num] # filter for the indices and only get as many as we need\n",
    "labels = y_train[indices_for_class][:num]\n",
    "original_index =indices_for_class[:num]   # keep original index, so that we can find the items again\n",
    "\n",
    "# plot images\n",
    "fig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))\n",
    "for i in range(num):\n",
    "    if num_row>1 and num_col>1:\n",
    "        ax = axes[i//num_col, i%num_col]\n",
    "    elif num >1:\n",
    "        ax = axes[i]\n",
    "    else: ax=axes\n",
    "    #(image,ax,title,show_minor=False,show_major=False):\n",
    "    custom_imshow(images[i].reshape(28,28),ax,f'Item: {original_index[i]}\\n Correct Label: {labels[i]}\\n{class_names_dict[labels[i]]}',show_minor=False,show_major=False)\n",
    "fig.suptitle(f'Sample of items from predicted cluster {cluster_number}', fontsize=16)\n",
    " \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we suspected, the cluster with the predicted label `6`, which are trousers, works quite well. Other clusters like e.g. `5` is a wild mixture of sandals, ankle boots, coats, pullovers and more. \n",
    "\n",
    "A nice property of K-Means is, that it computes for each cluster a cluster center (centroid). This centroid is composed of the mean values of all items belonging to that cluster. When we visualise the cluster centroids, we will see the \"mean\" picture of all observations belonging to a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the number of rows and columns you want to see\n",
    "num_row = 3\n",
    "num_col = 5\n",
    "\n",
    "# get a segment of the dataset\n",
    "num = min(num_row*num_col,len(kmeans.cluster_centers_))\n",
    "images = kmeans.cluster_centers_\n",
    "\n",
    "# plot images\n",
    "fig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))\n",
    "for i in range(num_row*num_col):\n",
    "    if num_row>1 and num_col>1:\n",
    "        ax = axes[i//num_col, i%num_col]\n",
    "    elif num >1:\n",
    "        ax = axes[i]\n",
    "    else: ax=axes\n",
    "    if i>num-1:\n",
    "        ax.axis('off')\n",
    "    else:\n",
    "        custom_imshow(images[i].reshape(28,28),ax,f'Centroid: {i}',show_minor=False,show_major=False)\n",
    "fig.suptitle(f'KMeans Clustercenter', fontsize=16)\n",
    " \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "With the example in this notebook we had the advantage of knowing the correct labels and the possibility to visualise the data nicely. Typically when using clustering, we are dealing with unsupervised learning, i.e. data **without** labels, which makes evaluating the performance a lot more difficult. \n",
    "\n",
    "In the next notebook, we will have a look at how to approach a clustering problem and evaluate the performance in situations where we don't have labels and cannot easily visualise our data. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
