{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Clustering with K-Means"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \"If the intelligence is a cake the bulk of the cake is unsupervised learning, the icing on the cake is supervised learning and the cherry on the cake is reinforcement learning.\" Yann LeCun (NIPS 2016)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will showcase how to cluster data with the K-Means algorithm. We will use scikit-learn's implementation and apply it to the fashion MNIST dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import rand_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "from yellowbrick.cluster import KElbowVisualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's define some colors (yaaay corporate identity!)\n",
    "NF_Nemo=\"#FF4A11\"\n",
    "NF_Granite=\"#252629\"\n",
    "NF_Foam=\"#FFFFFF\"\n",
    "\n",
    "NF_DarkNemo=\"#E5430F\"\n",
    "NF_LightNemo=\"#FFB7A0\"\n",
    "NF_Dark_Grey=\"#3F4145\"\n",
    "\n",
    "NF_Blue=\"#33A5FF\"\n",
    "NF_Green=\"#A7E521\"\n",
    "NF_Yellow=\"#FFE600\"\n",
    "NF_Purple=\"#696CFF\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Objectives:\n",
    "\n",
    "* short EDA (show data and label)\n",
    "* elbow plot\n",
    "* cluster sklearn kmeans \n",
    "* error analysis by showing pictures of clusters\n",
    "* How do you figure out if clustering worked, if you don't have pictures that are easy to inspect?\n",
    "\n",
    "Elbow with yellowbrick: dashed line is point of maximum curvature\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "The [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset contains 70000 28x28 greyscale images of fashion products from a dataset of Zalando article images. It consists of 10 different classes which 7000 images per category. It is freely available for training and testing machine learning applications and can be directly downloaded from Tensorflow.\n",
    "\n",
    "The dataset introduced by Zalando is a more contemporary version of the original MNIST dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data from Tensorflow\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "print('Fashion MNIST Dataset Shape:')\n",
    "print('X_train: ' + str(X_train.shape))\n",
    "print('y_test: ' + str(y_test.shape))\n",
    "print('X_test:  '  + str(X_test.shape))\n",
    "print('y_test:  '  + str(y_test.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we have 60,000 samples in the training set (-> X_train) with a dimension of 28x28 each. Additionally, we got 10,000 samples for the testing set(-> X_test) with the same dimensions.\n",
    "\n",
    "The Dataset was designed to be used for supervised learning, so we also get the correct labels (in this case class labels) for all of the samples in the training (-> y_train) and test (-> y_test) sets. Typically, clustering is used in unsupervised settings. Then, we wouldn't have the labels available. For the purpose of this meetup, however we will use them to evaluate how good our clustering is working.\n",
    "\n",
    "The correct class names are encoded as numbers 0-9 but we can use the following dictionary to turn it into human readable labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class labels\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "class_names_dict = dict(zip(range(10),class_names)) \n",
    "#Here, its not really necessary to turn the class names into a dictionary,\n",
    "#However, I still like to to it as dictionaries are more robust against changing the sequence unintentionally.\n",
    "\n",
    "display(class_names_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, as we can see we get some different items that most of us have in their wardrobes. Bet let's have a closer look at the data that we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'This is the first item: \\n\\n {X_train[0]} \\n\\n It has the dimension: {X_train[0].shape}. \\n It has the label {y_train[0]} assigned, which means it is a {class_names_dict[y_train[0]]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you recognise the boot from that numbers? Me neither. We know it is supposed to be a picture, so lets treat it like a picture instead. The numbers in the data (0 to 255) represent the color of each of the 28x28 pixels we have. So lets use matplotlibs [plt.imshow()](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html) for a better presentation - we just add some custom labels, colors etc. to improve the visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_imshow(image,ax,title,show_minor=False,show_major=False):\n",
    "    # Show image\n",
    "    im = ax.imshow(image,\n",
    "                    interpolation='none', \n",
    "                    vmin=0, \n",
    "                    vmax=255, \n",
    "                    aspect='equal')\n",
    "    \n",
    "    # Add title\n",
    "    ax.set_title(title)\n",
    "\n",
    "    # Add Major ticks\n",
    "    ax.set_xticks(np.arange(0, 28, 5))\n",
    "    ax.set_yticks(np.arange(0, 28, 5))\n",
    "\n",
    "    # Add Labels for major ticks\n",
    "    ax.set_xticklabels(np.arange(0, 28, 5))\n",
    "    ax.set_yticklabels(np.arange(0, 28, 5))\n",
    "\n",
    "    # Add Minor ticks (for the minor grid)\n",
    "    ax.set_xticks(np.arange(-.5, 28, 1), minor=True)\n",
    "    ax.set_yticks(np.arange(-.5, 28, 1), minor=True)\n",
    "    \n",
    "    # Remove Minor ticks\n",
    "    ax.tick_params(which='minor', bottom=False, left=False)\n",
    "    \n",
    "    # Add (or don't) the selected grid lines\n",
    "    if(show_minor):\n",
    "        ax.grid(which='minor', color=NF_Blue, linestyle='-', linewidth=1)\n",
    "    else:\n",
    "        ax.grid(which='minor',visible=False)\n",
    "    if(show_major):\n",
    "        ax.grid(which='major', color=NF_Nemo, linestyle='-', linewidth=1)\n",
    "    else:\n",
    "        ax.grid(which='major',visible=False)\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we get a proper visualisation of our sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'This is the first item:')\n",
    "plt.figure(figsize=(8,8))\n",
    "ax=plt.gca()\n",
    "custom_imshow(X_train[0],ax,title=\"\",show_minor=True,show_major=True)\n",
    "plt.show()\n",
    "print(f'It has the dimension: {X_train[0].shape}. \\nIt has the label {y_train[0]} assigned, which means it is a {class_names_dict[y_train[0]]}.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that starts to make more sense! Let's look at some more observations!\n",
    "\n",
    "Do you notice something that could be problematic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First fix random seed so that everyone using this sees the same picture. Feel free to change it though!\n",
    "random_seed=42\n",
    "\n",
    "# specify the number of rows and columns you want to see\n",
    "num_row = 3\n",
    "num_col = 5\n",
    "\n",
    "# Here we compute how many samples we want:\n",
    "num = num_row*num_col \n",
    "\n",
    "# and here we use sklearns resample function to get us the samples.\n",
    "images, labels, original_index=resample(X_train,\n",
    "                                        y_train,\n",
    "                                        range(len(y_train)),\n",
    "                                        n_samples=num,\n",
    "                                        random_state=random_seed) \n",
    "\n",
    "# plot images\n",
    "fig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))\n",
    "for i in range(num):\n",
    "    if num_row>1 and num_col>1:\n",
    "        ax = axes[i//num_col, i%num_col]\n",
    "    elif num >1:\n",
    "        ax = axes[i]\n",
    "    else: ax=axes\n",
    "    #(image,ax,title,show_minor=False,show_major=False):\n",
    "    custom_imshow(images[i],ax,f'Item: {original_index[i]}\\nLabel: {labels[i]}\\n{class_names_dict[labels[i]]}',show_minor=False,show_major=False)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the items, e.g., Ankle boots and trousers look pretty different that should be fine. Others, e.g. shirts, pullovers and coats, however,look pretty similar. Let's see if our clustering algorithm will be able to separate them correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's see how different items from each of the different classes look like: Is there something coming to your mind?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select which class to look at\n",
    "class_number=8\n",
    "\n",
    "# specify the number of rows and columns you want to see\n",
    "num_row = 2\n",
    "num_col = 5\n",
    "\n",
    "# Here we compute how many samples we want:\n",
    "num = num_row*num_col \n",
    "\n",
    "indices_for_class=np.argwhere(y_train==class_number).flatten() #get all the indices that are samples of the class we are interested in\n",
    "images = X_train[indices_for_class][:num] # filter for the indices and only get as many as we need\n",
    "labels = y_train[indices_for_class][:num]\n",
    "original_index =indices_for_class[:num]   # keep original index, so that we can find the items again\n",
    "\n",
    "# plot images\n",
    "fig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))\n",
    "for i in range(num):\n",
    "    if num_row>1 and num_col>1:\n",
    "        ax = axes[i//num_col, i%num_col]\n",
    "    elif num >1:\n",
    "        ax = axes[i]\n",
    "    else: ax=axes\n",
    "    #(image,ax,title,show_minor=False,show_major=False):\n",
    "    custom_imshow(images[i],ax,f'Item: {original_index[i]}\\nLabel: {labels[i]}\\n{class_names_dict[labels[i]]}',show_minor=False,show_major=False)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the classes, e.g. the bags (label 8) contain items that look pretty dissimilar. On the other hand some classes look much more uniform, e.g. trousers (label 1)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process Data for clustering\n",
    "\n",
    "After getting a feeling for the data we are dealing with, we can start to bring the data into the correct format. Scikit-learn's `K-Means` implementation expects the data to be an array with the shape (n_samples, n_features). Currently our data has one dimension to much, so we will need to flatten the pixels for each picture changing the shape from 28 x 28 (width x height) to a flat vector of 784 pixels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current shape of data (number of sample, width, height)\n",
    "print(\"Current shape X_train: \", X_train.shape)\n",
    "print(\"Current shape X_test:  \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping data using numpy\n",
    "X_train_flat = X_train.reshape(60000, -1)\n",
    "X_test_flat = X_test.reshape(10000, -1)\n",
    "\n",
    "print(\"New shape X_train: \", X_train_flat.shape)\n",
    "print(\"New shape X_test:  \", X_test_flat.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize what happened when we reshaped our data. The following plots compare the original 2-dimensional picture of one observation with the flattened array. You can think of it as cutting the picture horizontally in thin stripes and glueing them together to form a long stripe of pixels. \n",
    "\n",
    "> Note: To be able to see the color difference in the flattened image, we changed the aspect ratio. Ech pixel is not shown as a square but stretched vertically. If you want to display the original line of square pixels, you can change the parameter `aspect=\"auto\"` to `aspect=\"equal\"`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to see another observation change the value of the observation variable\n",
    "observation = 42\n",
    "\n",
    "fig, ax = plt.subplot_mosaic(\"A..;A..;A..;A..;BBB\", figsize=(15, 6))\n",
    "ax['A'].imshow(X_train[observation], cmap='gray', vmin=0, vmax=255)\n",
    "ax['B'].imshow(X_train_flat[observation].reshape(-1,1).T, aspect='auto',  cmap='gray', vmin=0, vmax=255)\n",
    "plt.suptitle(\"Comparison between 2-D image and flattened image of the same observation\")\n",
    "plt.subplots_adjust(hspace=0.5);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our data has the correct shape, we can continue with the actual clustering process. Before we train our model, let's draw a random sample that consists of 6000 of the original 60000 pictures of the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw random sample from training data using scikit-learn's resample function\n",
    "X_sample_flat, y_sample = resample(X_train_flat, y_train, n_samples=6000, replace=False, stratify=y_train, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print shape of sample data\n",
    "print(\"X_sample shape: \", X_sample_flat.shape)\n",
    "print(\"y_sample shape: \", y_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print distribution of labels?\n",
    "#pd.Series(y_sample).value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply K-Means and Elbow Method\n",
    "\n",
    "Finally we can cluster our data using K-Means. As you might remember, after initializing the cluster centroids K-Means alternates between two steps: \n",
    "1. assigning the data points to the nearest cluster centroid and \n",
    "2. updating the location of the centroids by calculating the mean of the data points assigned to each cluster \n",
    "\n",
    "until the assignment of the data points to the clusters doesn't change anymore. \n",
    "\n",
    "One major drawback of the algorithm is, that it cannot determine the amount of clusters itself. We need to define it when we fit the model. Sometimes the amount of clusters in our data set is given by the specific business case we are trying to solve (e.g. clustering customer data into a certain amount of groups), or we can gain some intuition during the EDA. But it can also happen, that we simply don't know what an appropriate amount of clusters would be. In those cases the elbow method can help us to select a value for k. \n",
    "\n",
    "In the next cell we will use the [yellowbrick](https://www.scikit-yb.org/en/latest/) package to create an elbow plot. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> Note: It might seem counterintuitive that we use a method to determine the number of clusters in our data even if we already know that we have ten different categories. Keep in mind, that this is usually not the case! Clustering is a domain of **unsupervised learning**, where we have to deal with **unlabeled** data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use elbow method with sample dataset to determine value for k \n",
    "kmeans_elbow_method = KMeans(init=\"k-means++\", n_init=\"auto\", random_state=1)\n",
    "visualizer_elbow_method = KElbowVisualizer(kmeans_elbow_method, k=(5,20))\n",
    "\n",
    "# Fit visualizer to sample data and display plot\n",
    "visualizer_elbow_method.fit(X_sample_flat)   \n",
    "visualizer_elbow_method.show();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yellowbrick's elbow plot indicates with the dashed black line, that `k=11` is the best amount of clusters for our data. It is the point of maximum curvature of the blue curve showing the distortion score (aka. within-cluster sum of squares). Let's adopt the suggestion and fit `KMeans()` with 11 cluster. Besides the parameter for the amount of clusters `n_clusters` we can also define which initialisation method for placing the initial cluster centroids we want to use. We will use the default value `init=k-means++`, which assures that the centroids get placed as far away form each other as possible mitigating the risk of accidentally splitting a cluster in half. With the third parameter `n_init`, we can control how many times K-Means will be run with different starting position for the centroids. Using `auto` in combination with `k-means++` results in one run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train K-Means on whole training data\n",
    "kmeans = KMeans(n_clusters=11 , init=\"k-means++\", n_init=\"auto\")\n",
    "kmeans.fit(X_train_flat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Performance and Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = kmeans.predict(X_train_flat)\n",
    "y_pred_test = kmeans.predict(X_test_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rand_score(y_train, y_pred_train))\n",
    "print(rand_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With random scores of 88.5%, generally we can say that the clustering is working well for most of the observations. We do know already, however, that while originally we had 10 classes but our clustering is trained using 10 clusters.\n",
    "\n",
    "Also, during the initial EDA, we suspected that some classes might be harder to seperate then others. Let's see if we were correct with that assumption!\n",
    "\n",
    "Since Kmeans is an unsupervised learning method, it randomly applies clusters numbers. Let's see, how well they match the classes we had to begin with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(y_pred_train,color=NF_Granite)#.value_counts()\n",
    "ax=plt.gca()\n",
    "sns.lineplot(x=[0,10],y=[6000,6000],ax=ax, c=NF_Nemo,ls='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now, that in our train dataset where 6000 observation for each of the classes. Our Clustering approach found most of the observations belonging to cluster 3, and least to cluster 10. This is already a good indication that our model was not able to seperate the classes correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's see, were the different observations went by looking at a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# Plot the confusion matrix\n",
    "mat = confusion_matrix(y_train, y_pred_train)\n",
    "sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False)\n",
    "plt.xlabel('predicted label')\n",
    "plt.ylabel('true label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the clustering not working correctly, the assigned labels are permutated. For example the predicted cluster 8, shows almost exclusively observations with the true label 0. To correct this, we can use np.argmin to select for each column the highest count to generate a \"translation dictionary\" between the assigned cluster labels and the original true labels. If we apply this, the resulting confusion matrix looks at least a bit better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred -> true\n",
    "'''pred_dict={\n",
    "8:0,\n",
    "1:1,\n",
    "3:2,\n",
    "9:3,\n",
    "0:4,\n",
    "6:5,\n",
    "5:6,\n",
    "4:7,\n",
    "10:8,\n",
    "7:9,\n",
    "2:10}'''\n",
    "old_mat = confusion_matrix(y_train, y_pred_train)\n",
    "pred_dict= dict(zip(range(11),np.argmax(old_mat,axis=0)))\n",
    "\n",
    "mat = confusion_matrix(y_train, [pred_dict[i] for i in y_pred_train])\n",
    "sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False)\n",
    "plt.xlabel('predicted label')\n",
    "plt.ylabel('true label');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select which cluster to look at -> predicted label != class label!\n",
    "cluster_number=10\n",
    "\n",
    "# specify the number of rows and columns you want to see\n",
    "num_row = 4\n",
    "num_col = 5\n",
    "\n",
    "# Here we compute how many samples we want:\n",
    "num = num_row*num_col \n",
    "\n",
    "indices_for_class=np.argwhere(y_pred_train==cluster_number).flatten() #get all the indices that are samples of the class we are interested in\n",
    "images = X_train_flat[indices_for_class][:num] # filter for the indices and only get as many as we need\n",
    "labels = y_train[indices_for_class][:num]\n",
    "original_index =indices_for_class[:num]   # keep original index, so that we can find the items again\n",
    "\n",
    "# plot images\n",
    "fig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))\n",
    "for i in range(num):\n",
    "    if num_row>1 and num_col>1:\n",
    "        ax = axes[i//num_col, i%num_col]\n",
    "    elif num >1:\n",
    "        ax = axes[i]\n",
    "    else: ax=axes\n",
    "    #(image,ax,title,show_minor=False,show_major=False):\n",
    "    custom_imshow(images[i].reshape(28,28),ax,f'Item: {original_index[i]}\\n Correct Label: {labels[i]}\\n{class_names_dict[labels[i]]}',show_minor=False,show_major=False)\n",
    "fig.suptitle(f'Sample of items from predicted cluster {cluster_number}', fontsize=16)\n",
    " \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As predicted, cluster \"1\" -> what we call trousers, seems to we working well. Others like \"3\" (shirts, tshirts pullovers, dresses) not so much.\n",
    "\n",
    "One nice property of kmeans is, that we can see the cluster center, i.e. the mean value of all items belonging to that cluster. Lets visualise these as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kmeans.inertia_\n",
    "#centroid = kmeans.cluster_centers_[5].reshape(28,28)\n",
    "# specify the number of rows and columns you want to see\n",
    "num_row = 3\n",
    "num_col = 5\n",
    "\n",
    "# get a segment of the dataset\n",
    "num = min(num_row*num_col,len(kmeans.cluster_centers_))\n",
    "images = kmeans.cluster_centers_\n",
    "#labels = y_train[:num]\n",
    "\n",
    "# plot images\n",
    "fig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))\n",
    "for i in range(num_row*num_col):\n",
    "    if num_row>1 and num_col>1:\n",
    "        ax = axes[i//num_col, i%num_col]\n",
    "    elif num >1:\n",
    "        ax = axes[i]\n",
    "    else: ax=axes\n",
    "    if i>num-1:\n",
    "        ax.axis('off')\n",
    "    else:\n",
    "        custom_imshow(images[i].reshape(28,28),ax,f'Centroid: {i}',show_minor=False,show_major=False)\n",
    "fig.suptitle(f'KMeans Clustercenter', fontsize=16)\n",
    " \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we had the advantage of knowing the correct labels (and the possibily to visualise this). Typically in clustering, we are dealing with unsupervised learning, i.e. data without labels. In the next notebook, we will learn to evaluate the performance of our clustering when we dont have the labels available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
