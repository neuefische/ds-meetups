{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Approaches to Consumer Credit Card Data #"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To open this notebook in Google Colab and start coding, click on the Colab icon below.\n",
    "\n",
    "<table style=\"border:2px solid orange\" align=\"left\">\n",
    "  <td style=\"border:2px solid orange \">\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/neuefische/ds-meetups/blob/main/05_Clustering/02_Clustering_credit_card_data.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    " </table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "from numpy import unique\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import davies_bouldin_score, silhouette_score, calinski_harabasz_score\n",
    "\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "from yellowbrick.features import rank2d\n",
    "\n",
    "from kneed import KneeLocator\n",
    "#InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Objectives: ###\n",
    "* Short EDA for consumer credit card data\n",
    "* KMeans clustering with several metrics and graphs for interpretation\n",
    "* DBSCAN clustering and graphs for interpretation\n",
    "* Hierarchical (Agglomerative) clustering and graphs for interpretation\n",
    "* Comparing approaches using several metrics: which approach is the best?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n",
    "\n",
    "The data for this notebook can be found on [Kaggle](https://www.kaggle.com/datasets/arjunbhasin2013/ccdata). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data into a pandas DataFrame\n",
    "df_cc = pd.read_csv(\"https://raw.githubusercontent.com/neuefische/ds-meetups/main/05_Clustering/data/credit_card_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see what the data looks like\n",
    "\n",
    "df_cc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check info on DataFrame. What data types are we dealing with? What steps might be necessary?\n",
    "\n",
    "df_cc.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see if there are missing values\n",
    "\n",
    "df_cc.isna().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we see MINIMUM_PAYMENTS and CREDIT_LIMIT have missing values. What do we do with these? Drop? Impute? \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If imputing missing values, it is important to look at things such as skewness of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df_cc['MINIMUM_PAYMENTS'])\n",
    "plt.xlabel('MINIMUM_PAYMENTS')\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MINIMUM_PAYMENTS is heavily skewed with high kurtosis. This means many outliers. In cases such as this, it is best to use median. Consider mode if results are not good.\n",
    "\n",
    "Other key points are that the number of missings is less than 5%, so going with median should not create other problems. \n",
    "\n",
    "The median value should be generated from the entire data set. \n",
    "\n",
    "[Here](https://medium.com/analytics-vidhya/feature-engineering-part-1-mean-median-imputation-761043b95379) you can read more on mean/median imputation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cc['MINIMUM_PAYMENTS'].fillna(df_cc['MINIMUM_PAYMENTS'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop record with missing credit limit? Alternatively, one could impute with mean or median.\n",
    "\n",
    "df_cc['CREDIT_LIMIT'].fillna(df_cc['CREDIT_LIMIT'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check again for missings\n",
    "\n",
    "df_cc.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check correlations: this is potentially important when clustering\n",
    "\n",
    "\n",
    "_, ax = plt.subplots(ncols=1, figsize=(8,4))\n",
    "\n",
    "#rank1d(df_cc.drop(columns=['CUST_ID']), ax=ax, show=False)\n",
    "rank2d(df_cc.drop(columns=['CUST_ID']), ax=ax, show=False)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High correlations exist between many variables:\n",
    "\n",
    "PURCHASES, INSTALLMENTS_PURCHASES, ONEOFF_PURCHASES, ONEOFF_PURCHASES_FREQUENCY, PURCHASES_TRX, CREDIT_LIMIT, MINIMUM_PAYMENTS\n",
    "\n",
    "BALANCE, CASH_ADVANCE, CREDIT_LIMIT\n",
    "\n",
    " ONEOFF_PURCHASES_FREQUENCY\n",
    "\n",
    "\n",
    "\n",
    "PURCHASES_INSTALLMENTS, PURCHASES_INSTALLMENTS_FREQUENCY,  \n",
    "CASH_ADVANCE_FREQUENCY CASH_ADVANCE_TRX"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discussions I have seen indicate that for clustering, PCA is the best approach in this case. Before doing this, get rid of any columns we don't need.\n",
    "\n",
    "If you are not familiar with principle component analysis (PCA), you can find a visual explanation [here](https://setosa.io/ev/principal-component-analysis/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop CUST_ID due to high cardinality\n",
    "\n",
    "df_cc_drop = df_cc.drop('CUST_ID', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling the data is important for clustering\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df_cc_scaled_pre_pca = scaler.fit_transform(df_cc_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do PCA, as this tends to help the respective clustering algorithms and is a best practice. First fit the data\n",
    "\n",
    "pca = PCA()\n",
    "\n",
    "pca.fit(df_cc_scaled_pre_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as a step in evaluating how to conduct PCA, we need to generate the explained variance ratios. \n",
    "\n",
    "explained_variance_pca = pca.explained_variance_ratio_\n",
    "print(explained_variance_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get the values into a form that is useful for plotting, we then need to take the cumulative sum of the explained variances\n",
    "\n",
    "cumulative_sum_eigenvalues = np.cumsum(explained_variance_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the cumulative sum of the explained variances can be plotted\n",
    "\n",
    "plt.figure(figsize = (10,8))\n",
    "\n",
    "plt.plot(range(0,len(cumulative_sum_eigenvalues)), cumulative_sum_eigenvalues, marker = 'x', linestyle = '--')\n",
    "plt.title('Explained Variance by Components')\n",
    "plt.axhline(y=.85, linestyle='dashdot', color='r')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For PCA, a good rule of thumb is to preserve around 85% of the variance. This would mean keeping 7 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=7)\n",
    "pca.fit(df_cc_scaled_pre_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cc_pca = pca.transform(df_cc_scaled_pre_pca)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-MEANS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use elbow method with sample dataset to determine value for k \n",
    "kmeans_elbow_method = KMeans(init=\"k-means++\", n_init=\"auto\", random_state=1)\n",
    "visualizer_elbow_method = KElbowVisualizer(kmeans_elbow_method, k=(2,9))\n",
    "\n",
    "# Fit visualizer to sample data and display plot\n",
    "visualizer_elbow_method.fit(df_cc_pca)   \n",
    "visualizer_elbow_method.show();\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the plot and these values, for K-means we will go with 4 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run K-means for 4 clusters\n",
    "\n",
    "kmeans_pca = KMeans(n_clusters=4, init='k-means++', n_init=10, random_state=42)\n",
    "kmeans_pca.fit(df_cc_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare original set to be joined with cluster results\n",
    "\n",
    "df_clustered_kmeans = df_cc_drop.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the pertinent k-means labels\n",
    "\n",
    "df_clustered_kmeans['Segment KMeans'] = kmeans_pca.labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "\n",
    "df_clustered_kmeans.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the columns by the various columns to see what the separation achieves and for interpretation. \n",
    "\n",
    "for c in df_clustered_kmeans:\n",
    "    grid= sns.FacetGrid(df_clustered_kmeans, col='Segment KMeans')\n",
    "    grid.map(plt.hist, c)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN\n",
    "\n",
    "Time to check out DBSCAN. This approach differs from K-means, in that the ideal number of clusters is not specified up front, but rather other parameters. These are epsilon and the minimum number of points to be sampled in each cluster. \n",
    "\n",
    "Before carrying out the clustering, we will walk through what is needed to find good epsilon (eps) and min_points.\n",
    "\n",
    "We can use the PCA DataFrame for DBSCAN clustering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter tuning for eps. This involves another form of elbow plot\n",
    "\n",
    "nearest_neighbors = NearestNeighbors(n_neighbors=11)\n",
    "neighbors = nearest_neighbors.fit(df_cc_pca)\n",
    "distances, indices = neighbors.kneighbors(df_cc_pca)\n",
    "distances = np.sort(distances[:,10], axis=0)\n",
    "\n",
    "i = np.arange(len(distances))\n",
    "knee = KneeLocator(i, distances, S=1, curve='convex', direction='increasing', interp_method='polynomial')\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "knee.plot_knee()\n",
    "plt.axhline(distances[knee.knee], label='optimal epsilon',linestyle='dashdot', color='r')\n",
    "plt.xlabel(\"Points\")\n",
    "plt.ylabel(\"Distance\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"The optimal epsilon is the red dash dot line with a value of \", distances[knee.knee])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find the optimum number of min_points, we can search for the highest silhouette score in a range of values \n",
    "\n",
    "epsilon = 1.5125394356711748\n",
    "\n",
    "min_samples = range(3,10)\n",
    "for min_sample in min_samples:\n",
    "    dbs = DBSCAN(eps=epsilon, min_samples=min_sample)\n",
    "    dbs.fit(df_cc_pca)\n",
    "    print(f\"Silhouette Score for epsilon {epsilon} and minimum number of points {min_sample} is\", silhouette_score(df_cc_pca, dbs.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take epsilon 1.5125394356711748 and number of samples 5 as optimum\n",
    "\n",
    "dbs=DBSCAN(eps=1.5125394356711748, min_samples=5)\n",
    "dbs.fit(df_cc_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the original data set to be merged with the values of the clusters\n",
    "\n",
    "df_clustered_dbscan = df_cc_drop.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the cluseter values to the various points\n",
    "\n",
    "df_clustered_dbscan['Segment DBSCAN'] = dbs.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the columns in the respective clusters for interpretation. What do you think about the effectiveness of DBSCAN? \n",
    "# Are the clusters well-separated? What information do they yield about the credit card users?\n",
    "\n",
    "for c in df_clustered_dbscan:\n",
    "    grid= sns.FacetGrid(df_clustered_dbscan, col='Segment DBSCAN')\n",
    "    grid.map(plt.hist, c)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical (Agglomerative) Clustering\n",
    "\n",
    "The third kind of clustering algorithm is a form of hierarchical clustering. There are two forms of hierarchical clustering--agglomerative and divisive--, but we will only look at agglomerative clustering. \n",
    "\n",
    "Herarchical clusters work by starting with each data point as a cluster, and then joining points that are nearest. This generates a dendrogram that in itself works to group points with commonalities. \n",
    "\n",
    "Working with such a dendrogram, it is possible to identify an ideal number of clusters by finding the highest horizontal line that cuts across the verticals at the same level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code generates a dendrogram for the PCA dataFrame. \n",
    "# RUN THIS CELL ONCE AS IS, IF IT WOULD BE HELPFUL TO SEE WHERE THE HORIZONTAL CUT CAN BE MADE, THEN RUN WITH THE AXHLINE CODE UNCOMMENTED\n",
    "\n",
    "hierarchical_cluster = sch.linkage(df_cc_pca, method = 'ward')\n",
    "plt.title('Dendrogram', fontsize = 20)\n",
    "plt.ylabel('Euclidean  Distance')\n",
    "\n",
    "# CAN BE RUN WITH FOLLOWING LINE UNCOMMENTED\n",
    "#plt.axhline(y=122, color='r', linestyle='--')\n",
    "\n",
    "sch.dendrogram(hierarchical_cluster, truncate_mode = \"level\", p = 7, show_leaf_counts = False, no_labels = True)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line indicated by this technique gives us 4 verticals, or four clusters to start with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# working with 4 clusters \n",
    "\n",
    "hc = AgglomerativeClustering(n_clusters = 4, metric = 'euclidean', linkage = 'ward')\n",
    "y_hc = hc.fit(df_cc_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare original data to receive the HC clustering results\n",
    "\n",
    "df_clustered_hc = df_cc_drop.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the HC labels\n",
    "\n",
    "df_clustered_hc['Segment HC'] = y_hc.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph to check clustering and interpret results\n",
    "\n",
    "for c in df_clustered_hc:\n",
    "    grid= sns.FacetGrid(df_clustered_hc, col='Segment HC')\n",
    "    grid.map(plt.hist, c)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Algorithms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A final section looks at the respective metrics for the various algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means from sklearn.cluster import KMeans\n",
    "\n",
    "# Fit K-Means\n",
    "kmeans = KMeans(n_clusters=4, init='k-means++', n_init=10, random_state=42)\n",
    "\n",
    "# Use fit_predict to cluster the dataset\n",
    "y_predict = kmeans.fit_predict(df_cc_pca)\n",
    "\n",
    "# Calculate cluster validation metrics\n",
    "kmeans_scores = []\n",
    "kmeans_scores.append(silhouette_score(df_cc_pca, kmeans.labels_, metric='euclidean'))\n",
    "kmeans_scores.append(calinski_harabasz_score(df_cc_pca, kmeans.labels_))\n",
    "kmeans_scores.append(davies_bouldin_score(df_cc_pca, y_predict))\n",
    "\n",
    "\n",
    "# dbscan clustering by score\n",
    "model = DBSCAN(eps=1.5125394356711748, min_samples= 5)\n",
    "\n",
    "# fit model and predict clusters\n",
    "y_predict = model.fit_predict(df_cc_pca)\n",
    "\n",
    "# retrieve unique clusters\n",
    "clusters = unique(y_predict)\n",
    "\n",
    "\n",
    "# Calculate cluster validation metrics\n",
    "dbscan_scores = []\n",
    "dbscan_scores.append(silhouette_score(df_cc_pca, y_predict, metric='euclidean'))\n",
    "dbscan_scores.append(calinski_harabasz_score(df_cc_pca, y_predict))\n",
    "dbscan_scores.append(davies_bouldin_score(df_cc_pca, y_predict))\n",
    "\n",
    "# Agglomerative clustering evaluation by score\n",
    "model = AgglomerativeClustering(n_clusters=4)\n",
    "\n",
    "# fit model and predict clusters\n",
    "y_predict = model.fit(df_cc_pca)\n",
    "y_predict_2 = model.fit_predict(df_cc_pca)\n",
    "\n",
    "# retrieve unique clusters\n",
    "clusters = unique(y_predict)\n",
    " \n",
    "# Calculate cluster validation metrics\n",
    "hc_scores = []\n",
    "hc_scores.append(silhouette_score(df_cc_pca, y_predict.labels_, metric='euclidean'))\n",
    "hc_scores.append(calinski_harabasz_score(df_cc_pca, y_predict.labels_))\n",
    "hc_scores.append(davies_bouldin_score(df_cc_pca, y_predict_2))\n",
    "\n",
    "df_scores = pd.DataFrame(zip(kmeans_scores,dbscan_scores,hc_scores),\n",
    "                         index = ['Silhouette', 'Calinski Harabasz', 'Davies Bouldin'],\n",
    "                         columns = ['Kmeans', 'DBSCAN', 'Hierarchical']\n",
    "                         )\n",
    " \n",
    "df_scores.transpose().head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
