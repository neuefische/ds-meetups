{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Approaches to Consumer Credit Card Data #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "from numpy import unique\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import davies_bouldin_score, silhouette_score, calinski_harabasz_score\n",
    "\n",
    "from kneed import KneeLocator\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Objectives: ##\n",
    "* Short EDA for consumer credit card data\n",
    "* KMeans clustering with several metrics and graphs for interpretation\n",
    "* DBSCAN clustering and graphs for interpretation\n",
    "* Hierarchical (Agglomerative) clustering and graphs for interpretation\n",
    "* Comparing approaches using several metrics: which approach is the best?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to download and save data locally from \n",
    "# https://www.kaggle.com/datasets/arjunbhasin2013/ccdata/download?datasetVersionNumber=1\n",
    "# and then read it into a Pandas DataFrame \n",
    "\n",
    "with open(r\"CC_GENERAL.csv\") as csv_file:\n",
    "    df_cc = pd.read_csv(csv_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see what the data looks like\n",
    "\n",
    "df_cc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check info on DataFrame. What data types are we dealing with? What steps might be necessary?\n",
    "\n",
    "df_cc.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see if there are missing values\n",
    "\n",
    "df_cc.isna().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we see MINIMUM_PAYMENTS and CREDIT_LIMIT have missing values. What do we do with these? Drop? Impute? \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If imputing missing values, it is important to look at things such as skewness of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for skewness by looking at the distributions\n",
    "\n",
    "plt.figure(figsize=(20,35))\n",
    "for i, column in enumerate(df_cc.columns):\n",
    "    if df_cc[column].dtype != 'object':\n",
    "        ax = plt.subplot(9, 2, i+1)\n",
    "        sns.histplot(df_cc[column], ax=ax)\n",
    "        plt.xlabel(column)\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the overall data set, MINIMUM_PAYMENTS is heavily skewed with high kurtosis. This means many outliers. In cases such as this, it is best to use median. Consider mode if results are not good.\n",
    "\n",
    "Other key points are that the number of missings is less than 5%, so going with median should not create other problems. \n",
    "\n",
    "The median value should be genereated from the entire data set. \n",
    "\n",
    "https://medium.com/analytics-vidhya/feature-engineering-part-1-mean-median-imputation-761043b95379"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cc['MINIMUM_PAYMENTS'].fillna(df_cc['MINIMUM_PAYMENTS'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop record with missing credit limit? Alternatively, one could impute with mean or median. Mean value is $4495, median $3000. \n",
    "# if terms of time spent, the fastest choice would be to just drop the record.  \n",
    "\n",
    "print(\"Median:\", df_cc['CREDIT_LIMIT'].median())\n",
    "print(\"Mean:\", df_cc['CREDIT_LIMIT'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my impulse is to go with median, but you can try out mean\n",
    "\n",
    "df_cc['CREDIT_LIMIT'].fillna(df_cc['CREDIT_LIMIT'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check again for missings\n",
    "\n",
    "df_cc.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check correlations: this is potentially important when clustering\n",
    "\n",
    "plt.figure(figsize=(9,16))\n",
    "sns.heatmap(df_cc.corr(), annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High correlations exist between many variables\n",
    "\n",
    "PURCHASES, INSTALLMENTS_PURCHASES, ONEOFF_PURCHASES, ONEOFF_PURCHASES_FREQUENCY, PURCHASES_TRX, CREDIT_LIMIT, MINIMUM_PAYMENTS\n",
    "\n",
    "BALANCE, CASH_ADVANCE, CREDIT_LIMIT\n",
    "\n",
    " ONEOFF_PURCHASES_FREQUENCY\n",
    "\n",
    "\n",
    "\n",
    "PURCHASES_INSTALLMENTS, PURCHASES_INSTALLMENTS_FREQUENCY,  \n",
    "CASH_ADVANCE_FREQUENCY CASH_ADVANCE_TRX"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discussions I have seen indicate that for clustering, PCA is the best approach in this case. Before doing this, get rid of any columns we don't need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop cust_id due to high cardinality\n",
    "\n",
    "df_cc_drop = df_cc.drop('CUST_ID', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe to feather in case of crashes, etc\n",
    "\n",
    "df_cc_drop.to_feather(r\"df_cc_drop.ftr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the dataset if needed\n",
    "\n",
    "with open(r\"df_cc_drop.ftr\", \"rb\") as feather_file:\n",
    "    df_cc_drop = pd.read_feather(feather_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling the data is important for clustering\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df_cc_scaled_pre_pca = scaler.fit_transform(df_cc_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do PCA, as this tends to help the respective clustering algorithms and is a best practice. First fit the data\n",
    "\n",
    "pca = PCA()\n",
    "\n",
    "pca.fit(df_cc_scaled_pre_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as a step in evaluating how to conduct PCA, we need to generate the explained variance ratios. \n",
    "\n",
    "explained_variance_pca = pca.explained_variance_ratio_\n",
    "print(explained_variance_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get the values into a form that is usefull for plotting, we then need to take the cummulative sum of the explained variances\n",
    "\n",
    "cumulative_sum_eigenvalues = np.cumsum(explained_variance_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the cummulative sum of the explained variances can be plotted\n",
    "\n",
    "plt.figure(figsize = (10,8))\n",
    "\n",
    "plt.plot(range(0,len(cumulative_sum_eigenvalues)), cumulative_sum_eigenvalues, marker = 'x', linestyle = '--')\n",
    "plt.title('Explained Variance by Components')\n",
    "plt.axhline(y=.85, linestyle='dashdot', color='r')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For PCA, a good rule of thumb is to preserve around 85% of the variance. This would mean keeping 7 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=7)\n",
    "pca.fit(df_cc_scaled_pre_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cc_pca = pca.transform(df_cc_scaled_pre_pca)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # K-MEANS #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do an elbow graph for k-means on the transformed data, first calculate the kmeans for a range of clusters. \n",
    "# Check other metrics at the same time for comparison\n",
    "\n",
    "wcss_values = []\n",
    "silhouette_list = []\n",
    "davies_bouldin_list = []\n",
    "calinski_harabasz_list = []\n",
    "\n",
    "for number_of_clusters in range(2,9):\n",
    "    kmeans_pca = KMeans(n_clusters=number_of_clusters, init='k-means++', n_init=10, random_state=42)\n",
    "    kmeans_pca.fit(df_cc_pca)\n",
    "    \n",
    "    wcss_values.append(kmeans_pca.inertia_)\n",
    "    labels = kmeans_pca.fit_predict(df_cc_pca)\n",
    "    silhouette_list.append(silhouette_score(df_cc_pca, labels))\n",
    "    davies_bouldin_list.append(davies_bouldin_score(df_cc_pca, labels))\n",
    "    calinski_harabasz_list.append(calinski_harabasz_score(df_cc_pca, labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.plot(range(2,9), wcss_values, marker='x', linestyle='-')\n",
    "plt.title('Elbow Curve of Scores by Cluster')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('K-means with PCA Clustering')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elbow plot suggests 3, 4 or 5. But lets check other measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_values = range(2,9)\n",
    "\n",
    "plt.plot(x_values,silhouette_list, marker='X')\n",
    "plt.plot(x_values, davies_bouldin_list, marker='o')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Davies Bouldin indicates 7, which is not so clearly optimal given that we only have 7 principal components. Silhouette indicates 3, but the differences are rather small. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = range(2,9)\n",
    "df_cali_har = pd.DataFrame(x_values,columns=['x_values'])\n",
    "df_cali_har['cali_har'] = calinski_harabasz_list\n",
    "\n",
    "max_x = df_cali_har.loc[df_cali_har['cali_har']==np.max(calinski_harabasz_list), 'x_values'].iloc[0]\n",
    "plt.plot(x_values,calinski_harabasz_list, marker='X')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Calinski Harabasz Score indicates best number of clusters is\", max_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is a numerical way to generate the best number of clusters using the package Kneedle\n",
    "\n",
    "wcss=[]\n",
    "for number_of_clusters in range(2,9):\n",
    "    kmeans_kneedle=KMeans(n_clusters=number_of_clusters, init='k-means++', n_init=10, random_state=42)\n",
    "    kmeans_kneedle.fit(df_cc_pca)\n",
    "    wcss_vl=kmeans_kneedle.inertia_\n",
    "    wcss.append(wcss_vl)\n",
    "kneedle = KneeLocator(np.arange(2,9), wcss, S=1.0, curve='convex', direction='decreasing')\n",
    "print(f'Kneedle indicates that {kneedle.knee} clusters is the optimum')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at different metrics, for K-means we will go with 4 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run K-means for 4 clusters\n",
    "\n",
    "kmeans_pca = KMeans(n_clusters=4, init='k-means++', n_init=10, random_state=42)\n",
    "kmeans_pca.fit(df_cc_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare original set to be joined with cluster results\n",
    "\n",
    "df_clustered_kmeans = df_cc_drop.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the pertinent k-means labels\n",
    "\n",
    "df_clustered_kmeans['Segment KMeans'] = kmeans_pca.labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "\n",
    "df_clustered_kmeans.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the columns by the various columns to see what the separation achieves and for interpretation. \n",
    "\n",
    "for c in df_clustered_kmeans:\n",
    "    grid= sns.FacetGrid(df_clustered_kmeans, col='Segment KMeans')\n",
    "    grid.map(plt.hist, c)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN #\n",
    "\n",
    "Time to check out DBSCAN. This approach differs from K-means, in that the ideal number of clusters is not specified up front, but rather other parameters. These are epsilon and the minimum number of points to be sampled in each cluster. \n",
    "\n",
    "Before carrying out the clustering, we will walk through what is needed to find good epsilon (eps) and min_points.\n",
    "\n",
    "We can use the PCA DataFrame for DBSCAN clustering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter tuning for eps. This involves another form of elbow plot\n",
    "\n",
    "nearest_neighbors = NearestNeighbors(n_neighbors=11)\n",
    "neighbors = nearest_neighbors.fit(df_cc_pca)\n",
    "distances, indices = neighbors.kneighbors(df_cc_pca)\n",
    "distances = np.sort(distances[:,10], axis=0)\n",
    "\n",
    "i = np.arange(len(distances))\n",
    "knee = KneeLocator(i, distances, S=1, curve='convex', direction='increasing', interp_method='polynomial')\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "knee.plot_knee()\n",
    "plt.axhline(distances[knee.knee], label='optimal epsilon',linestyle='dashdot', color='r')\n",
    "plt.xlabel(\"Points\")\n",
    "plt.ylabel(\"Distance\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"The optimal epsilon is the red dash dot line with a value of \", distances[knee.knee])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find the optimum number of min_points, we can search for the highest silhouette score in a range of values \n",
    "\n",
    "epsilon = 1.5125394356711748\n",
    "\n",
    "min_samples= range(3,10)\n",
    "for min_sample in min_samples:\n",
    "    dbs=DBSCAN(eps=epsilon, min_samples=min_sample)\n",
    "    dbs.fit(df_cc_pca)\n",
    "    print(f\"Silhouette Coefficient for epsilon {epsilon} and minimum number of points {min_sample} is\",silhouette_score(df_cc_pca, dbs.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take epsilon 1.5125394356711748 and number of samples 5 as optimum\n",
    "\n",
    "dbs=DBSCAN(eps=1.5125394356711748, min_samples=5)\n",
    "dbs.fit(df_cc_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the original data set to be merged with the values of the clusters\n",
    "\n",
    "df_clustered_dbscan = df_cc_drop.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the cluseter values to the various points\n",
    "\n",
    "df_clustered_dbscan['Segment DBSCAN'] = dbs.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the columns in the respective clusters for interpretation. What do you think about the effectiveness of DBSCAN? \n",
    "# Are the clusters well-separated? What information do they yield about the credit card users?\n",
    "\n",
    "for c in df_clustered_dbscan:\n",
    "    grid= sns.FacetGrid(df_clustered_dbscan, col='Segment DBSCAN')\n",
    "    grid.map(plt.hist, c)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical (Agglomerative) Clustering #\n",
    "\n",
    "The third kind of clustering algorithm is a form of hierarchical clustering. There are two forms of hierarchical clustering--agglomerative and divisive--, but we will only look at agglomerative clustering. \n",
    "\n",
    "Herarchical clusters work by starting with each data point as a cluster, and then joining points that are nearest. This generates a dendrogram that in itself works to group points with commonalities. \n",
    "\n",
    "Working with such a dendrogram, it is possible to identify an ideal number of clusters by finding the highest horizontal line that cuts across the verticals at the same level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code generates a dendrogram for the PCA dataFrame. \n",
    "# RUN THIS CELL ONCE AS IS, IF IT WOULD BE HELPFUL TO SEE WHERE THE HORIZONTAL CUT CAN BE MADE, THEN RUN WITH THE AXHLINE CODE UNCOMMENTED\n",
    "\n",
    "hierarchical_cluster = sch.linkage(df_cc_pca, method = 'ward')\n",
    "plt.title('Dendrogram', fontsize = 20)\n",
    "plt.ylabel('Euclidean  Distance')\n",
    "\n",
    "# CAN BE RUN WITH FOLLOWING LINE UNCOMMENTED\n",
    "# plt.axhline(y=122, color='r', linestyle='--')\n",
    "\n",
    "sch.dendrogram(hierarchical_cluster, truncate_mode = \"level\", p = 7, show_leaf_counts = False, no_labels = True)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line indicated by this technique gives us 4 verticals, or four clusters to start with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# working with 4 clusters \n",
    "\n",
    "hc = AgglomerativeClustering(n_clusters = 4, metric = 'euclidean', linkage = 'ward')\n",
    "y_hc = hc.fit(df_cc_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare original data to receive the HC clustering results\n",
    "\n",
    "df_clustered_hc = df_cc_drop.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the HC labels\n",
    "\n",
    "df_clustered_hc['Segment HC'] = y_hc.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph to check clustering and interpret results\n",
    "\n",
    "for c in df_clustered_hc:\n",
    "    grid= sns.FacetGrid(df_clustered_hc, col='Segment HC')\n",
    "    grid.map(plt.hist, c)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A final section looks at the respective metrics for the various algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means from sklearn.cluster import KMeans\n",
    "\n",
    "# Fit K-Means\n",
    "kmeans = KMeans(n_clusters=4, init='k-means++', n_init=10, random_state=42)\n",
    "\n",
    "# Use fit_predict to cluster the dataset\n",
    "y_predict = kmeans.fit_predict(df_cc_pca)\n",
    "\n",
    "# Calculate cluster validation metrics\n",
    "\n",
    "silhouette_kmeans = silhouette_score(df_cc_pca, kmeans.labels_, metric='euclidean')\n",
    "cali_har_kmeans = calinski_harabasz_score(df_cc_pca, kmeans.labels_)\n",
    "davies_bouldin_kmeans = davies_bouldin_score(df_cc_pca, y_predict)\n",
    "\n",
    "print('Silhouette Score: %.2f' % silhouette_kmeans)\n",
    "print('Calinski Harabasz Score: %.2f' % cali_har_kmeans)\n",
    "print('Davies Bouldin Score: %.2f' % davies_bouldin_kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dbscan clustering by score\n",
    "\n",
    "model = DBSCAN(eps=1.5125394356711748, min_samples= 5)\n",
    "\n",
    "# fit model and predict clusters\n",
    "\n",
    "y_predict = model.fit_predict(df_cc_pca)\n",
    "\n",
    "# retrieve unique clusters\n",
    "clusters = unique(y_predict)\n",
    "\n",
    "\n",
    "# Calculate cluster validation metrics\n",
    "\n",
    "silhouette_dbscan = silhouette_score(df_cc_pca, y_predict, metric='euclidean')\n",
    "\n",
    "cali_har_dbscan = calinski_harabasz_score(df_cc_pca, y_predict)\n",
    "\n",
    "davies_bouldin_dbscan = davies_bouldin_score(df_cc_pca, y_predict)\n",
    "\n",
    "\n",
    "\n",
    "print('Silhouette Score DBSCAN: %.2f' % silhouette_dbscan)\n",
    "print('Calinski Harabasz Score DBSCAN : %.2f' % cali_har_dbscan)\n",
    "print('Davies Bouldin Score DBSCAN: %.2f' % davies_bouldin_dbscan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agglomerative clustering evaluation by score\n",
    "\n",
    "model = AgglomerativeClustering(n_clusters=4)\n",
    "\n",
    "# fit model and predict clusters\n",
    "y_predict = model.fit(df_cc_pca)\n",
    "y_predict_2 = model.fit_predict(df_cc_pca)\n",
    "\n",
    "# retrieve unique clusters\n",
    "clusters = unique(y_predict)\n",
    " \n",
    "# Calculate cluster validation metrics\n",
    "\n",
    "silhouette_HC = silhouette_score(df_cc_pca, y_predict.labels_, metric='euclidean')\n",
    "cali_har_HC = calinski_harabasz_score(df_cc_pca, y_predict.labels_)\n",
    "davies_bouldin_HC = davies_bouldin_score(df_cc_pca, y_predict_2)\n",
    "\n",
    "print('Silhoutte Score for HC: %.2f' % silhouette_HC)\n",
    "print('Calinski Harabasz Score for HC: %.2f' % cali_har_HC)\n",
    "print('Davies Bouldin Score for HC: %.2f' % davies_bouldin_HC)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
